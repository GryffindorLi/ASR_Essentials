{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC implementation\n",
    "\n",
    "This notebook implements a CTC layer based on char model using Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "\n",
    "First, let's define some util functions, such as padding target transcripts, generating random CTC output, and the Softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_tgt_seq(tgt: str):\n",
    "    return f\"^{'^'.join(tgt.lower())}^\"\n",
    "\n",
    "def _random_ctc_output(T, N=28):\n",
    "    return np.random.random((T, N))\n",
    "\n",
    "def softmax(in_arr):\n",
    "    exp_arr = np.exp(in_arr)\n",
    "\n",
    "    return exp_arr / np.sum(exp_arr, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class CharModel:\n",
    "    chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    SPACE = \" \"\n",
    "    BLANK_SYMBOL = '^'\n",
    "    \n",
    "    char_to_ind = {char: ind for ind, char in enumerate(chars + SPACE + BLANK_SYMBOL)}\n",
    "    ind_to_char = {ind: char for ind, char in enumerate(chars + SPACE + BLANK_SYMBOL)}\n",
    "\n",
    "    Vocab_Size = len(chars) + 2\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(padded_seq: str):\n",
    "        return np.fromiter(map(\n",
    "                lambda x: CharModel.char_to_ind[x], [c for c in padded_seq.lower()]\n",
    "            ), \n",
    "            dtype=np.int64)\n",
    "    \n",
    "    @staticmethod\n",
    "    def decode(ind_seq: List[int]):\n",
    "        return list(map(lambda x: CharModel.ind_to_char[x], ind_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 26, 11,  8, 21,  4], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CharModel.encode(\"I live\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTC Layer\n",
    "\n",
    "Let's build [CTC](https://www.cs.toronto.edu/~graves/icml_2006.pdf) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTC:\n",
    "    def __init__(self, output, tgt):\n",
    "        self.tgt = tgt\n",
    "        self.output = output\n",
    "\n",
    "        self.padded_tgt = CharModel.encode(_pad_tgt_seq(tgt))\n",
    "\n",
    "        self.alpha = np.zeros((self.output.shape[0], self.padded_tgt.shape[0])) # forward variable alpha\n",
    "\n",
    "        self.beta = np.zeros((self.output.shape[0], self.padded_tgt.shape[0])) # backward variable beta\n",
    "\n",
    "        self.gd = np.zeros_like(output) # gradient\n",
    "\n",
    "    def _alpha(self, t, s):\n",
    "        \"\"\"formula (6) in Graves et al., 2006\n",
    "        \"\"\"\n",
    "        if s < 0 or s >= self.padded_tgt.shape[0]:\n",
    "            return 0\n",
    "        \n",
    "        curr_char = self.padded_tgt[s]\n",
    "        curr_score = self.output[t, curr_char]\n",
    "\n",
    "        if t == 0:\n",
    "            if s == 0:\n",
    "                return self.output[0, CharModel.char_to_ind[CharModel.BLANK_SYMBOL]]\n",
    "            elif s == 1:\n",
    "                return curr_score\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        alpha_tgt_t_s = self.alpha[t - 1, s] + (self.alpha[t - 1, s - 1] if s - 1 >= 0 else 0)\n",
    "\n",
    "        if curr_char == CharModel.BLANK_SYMBOL or (s >= 2 and self.padded_tgt[s - 2] == curr_char):\n",
    "            return alpha_tgt_t_s * curr_score\n",
    "        else:\n",
    "            return (alpha_tgt_t_s + (self.alpha[t - 1, s - 2] if s - 2 >= 0 else 0)) * curr_score     \n",
    "\n",
    "    def _beta(self, t, s):\n",
    "        \"\"\"formula (10) in Graves et al., 2006\n",
    "        \"\"\"\n",
    "        if s < 0 or s >= self.padded_tgt.shape[0]:\n",
    "            return 0\n",
    "        \n",
    "        curr_char = self.padded_tgt[s]\n",
    "        curr_score = self.output[t, curr_char]\n",
    "\n",
    "        last_time_step = self.output.shape[0] - 1\n",
    "        last_char_ind = self.padded_tgt.shape[0] - 1\n",
    "\n",
    "        if t == last_time_step:\n",
    "            if s == last_char_ind:\n",
    "                return self.output[t, CharModel.char_to_ind[CharModel.BLANK_SYMBOL]]\n",
    "            elif s == self.padded_tgt.shape[0] - 2:\n",
    "                return curr_score\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        beta_tgt_t_s = self.beta[t + 1, s] + (\n",
    "            self.beta[t + 1, s + 1] if s + 1 <= last_char_ind else 0\n",
    "        )\n",
    "\n",
    "        if curr_char == CharModel.BLANK_SYMBOL or \\\n",
    "            (s + 2 <= last_char_ind and self.padded_tgt[s + 2] == curr_char):\n",
    "            return beta_tgt_t_s * curr_score\n",
    "        else:\n",
    "            return (beta_tgt_t_s + \n",
    "                    self.beta[t + 1, s + 2] if s + 2 <= last_char_ind else 0\n",
    "                    ) * curr_score\n",
    "\n",
    "    def forward(self):\n",
    "        for t in range(self.alpha.shape[0]):\n",
    "            for s in range(self.alpha.shape[1]):\n",
    "                self.alpha[t, s] = self._alpha(t, s)\n",
    "\n",
    "    def backward(self):\n",
    "        for t in range(self.beta.shape[0] - 1, -1, -1):\n",
    "            for s in range(self.beta.shape[1] - 1, -1, -1):\n",
    "                self.beta[t, s] = self._beta(t, s)\n",
    "\n",
    "    def gradient(self):\n",
    "        \"\"\"formula (15) in Graves et al., 2006\n",
    "        \"\"\"\n",
    "        seq_prob = self.alpha[-1, -1] + self.alpha[-1, -2]\n",
    "        for t in range(self.output.shape[0]):\n",
    "            for k in range(self.output.shape[1]):\n",
    "                lab_l_k = np.argwhere(self.padded_tgt == k)\n",
    "                d_p_d_ytk = 0.0\n",
    "\n",
    "                if lab_l_k.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                for s in lab_l_k:\n",
    "                    d_p_d_ytk += self.alpha[t, s] * self.beta[t, s]\n",
    "                \n",
    "                d_p_d_ytk /= self.output[t, k] ** 2\n",
    "                d_logP_d_ytk = (1. / seq_prob) * d_p_d_ytk\n",
    "                self.gd[t, k] = d_logP_d_ytk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = _random_ctc_output(10)\n",
    "tgt = \"I am\"\n",
    "ctc = CTC(arr, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc.forward()\n",
    "ctc.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc.gradient()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
